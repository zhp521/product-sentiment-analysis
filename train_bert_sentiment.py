import torch
from transformers import (
    BertTokenizer, BertForSequenceClassification,
    Trainer, TrainingArguments, EvalPrediction,
    DataCollatorWithPadding
)
from datasets import Dataset, load_from_disk
import numpy as np
from sklearn.metrics import accuracy_score, f1_score, classification_report
import logging
import os

os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)# åªæœ‰INFOçº§åˆ«åŠä»¥ä¸Šçš„æ—¥å¿—æ¶ˆæ¯ä¼šè¢«è®°å½•ï¼ˆINFO, WARNING, ERROR, CRITICALï¼‰
logger = logging.getLogger(__name__)# åˆ›å»ºä¸€ä¸ªæ—¥å¿—è®°å½•å™¨(logger)å®ä¾‹

class SentimentAnalyzer:
    def __init__(self, model_name="bert-base-chinese"):
        self.model_name = model_name # åˆå§‹åŒ–BERTæ¨¡å‹å’Œåˆ†è¯å™¨
        self.tokenizer = BertTokenizer.from_pretrained(model_name)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.info(f"Using device: {self.device}")

        # å•†å“è¯„ä»·é¢†åŸŸå…³é”®è¯ï¼ˆç”¨äºå¢å¼ºæ¨¡å‹å…³æ³¨åº¦ï¼‰
        self.domain_keywords = [
            "ç‰©æµ", "å¿«é€’", "åŒ…è£…", "å®¢æœ", "æ€§ä»·æ¯”", "é€€è´§", "å‘è´§", "è´¨é‡", 
            "ä»·æ ¼", "æ•ˆæœ", "é€Ÿåº¦", "æœåŠ¡", "æ€åº¦", "æ­£å“", "å”®å", "ä½“éªŒ"
        ]

    # ç”¨äºè®¡ç®—æ¨¡å‹è¯„ä¼°æŒ‡æ ‡â€‹â€‹ï¼Œp: EvalPredictionâ€‹â€‹ä¸€ä¸ªåŒ…å«æ¨¡å‹é¢„æµ‹ç»“æœå’ŒçœŸå®æ ‡ç­¾çš„å¯¹è±¡ï¼Œé€šå¸¸åŒ…å«ä»¥ä¸‹å±æ€§ï¼š
    def compute_metrics(self, p: EvalPrediction):
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        preds = np.argmax(p.predictions, axis=1)
        accuracy = accuracy_score(p.label_ids, preds)
        f1 = f1_score(p.label_ids, preds, average='binary')
        
        return {
            "accuracy": accuracy,
            "f1": f1,
        }

    # é€šè¿‡ â€‹â€‹é¢†åŸŸå…³é”®è¯â€‹ å¢å¼ºæ¨¡å‹å¯¹ç‰¹å®šé¢†åŸŸå†…å®¹çš„å…³æ³¨ã€‚å°†æ–‡æœ¬è½¬åŒ–ä¸ºæ¨¡å‹å¯è¾“å…¥çš„å½¢å¼
    def tokenize_function(self, examples):
        """Tokenizeæ–‡æœ¬ï¼Œå¹¶æ·»åŠ é¢†åŸŸå…³é”®è¯çš„ç‰¹æ®Šæ ‡è®°"""
        # åŸºç¡€tokenization
        tokenized = self.tokenizer( # è°ƒç”¨åˆ†è¯å™¨å¤„ç†æ–‡æœ¬
            examples["text"],
            truncation=True,
            padding=False,
            max_length=512
        )
        
        # # ç®€å•çš„é¢†åŸŸå…³é”®è¯å¢å¼ºï¼šåœ¨å­˜åœ¨å…³é”®è¯çš„æ ·æœ¬ä¸Šæ·»åŠ ç‰¹æ®Šæ ‡è®°ï¼Œå®é™…ç”Ÿäº§ä¸­å¯é‡‡ç”¨æ›´å¤æ‚çš„æ³¨æ„åŠ›æ©ç æœºåˆ¶
        # for i, text in enumerate(examples["text"]):
        #     if any(keyword in text for keyword in self.domain_keywords):
        #         # è¿™é‡Œå¯ä»¥æ‰©å±•ä¸ºä¿®æ”¹attention maskï¼Œå¢åŠ å…³é”®è¯æƒé‡
        #         pass  # å ä½ç¬¦ï¼Œé«˜çº§å®ç°å¯åœ¨æ­¤å¤„ä¿®æ”¹
        
        return tokenized

    def train(self, train_dataset, eval_dataset, output_dir="./bert-sentiment-model"):
        """ä¸¤é˜¶æ®µè®­ç»ƒæµç¨‹"""
        
        # åˆå§‹åŒ–æ¨¡å‹
        model = BertForSequenceClassification.from_pretrained(
            self.model_name, 
            num_labels=2  # å‡è®¾æ˜¯äºŒåˆ†ç±»æƒ…æ„Ÿåˆ†æ
        )
        model.to(self.device)

        # æ•°æ®é¢„å¤„ç†
        logger.info("Tokenizing datasets...")
        tokenized_train = train_dataset.map(self.tokenize_function, batched=True)
        tokenized_eval = eval_dataset.map(self.tokenize_function, batched=True)

        # æ•°æ®æ”¶é›†å™¨ï¼ŒåŠ¨æ€å¡«å……æ‰¹æ¬¡å†…æ ·æœ¬åˆ°ç›¸åŒé•¿åº¦ï¼ˆä¼˜åŒ–æ˜¾å­˜ä½¿ç”¨ï¼‰
        data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)

        # è®­ç»ƒå‚æ•° - ç¬¬ä¸€é˜¶æ®µï¼šé€šç”¨æƒ…æ„Ÿå¾®è°ƒï¼ˆå†»ç»“åº•å±‚ï¼‰
        training_args_stage1 = TrainingArguments(
            output_dir=os.path.join(output_dir, "stage1"),# ç¬¬ä¸€é˜¶æ®µæ¨¡å‹å’Œæ—¥å¿—çš„ä¿å­˜è·¯å¾„
            # overwrite_output_dir=True,# å¦‚æœè¾“å‡ºç›®å½•å·²å­˜åœ¨ï¼Œåˆ™è¦†ç›–å®ƒï¼ˆé¿å…æ‰‹åŠ¨æ¸…ç†ï¼‰
            num_train_epochs=2,# è®­ç»ƒæ€»è½®æ¬¡ï¼ˆæ•´ä¸ªæ•°æ®é›†éå†2æ¬¡ï¼‰
            per_device_train_batch_size=16,# æ¯ä¸ªGPU/CPUâ€‹â€‹ çš„è®­ç»ƒæ‰¹æ¬¡å¤§å°ï¼ˆ16ä¸ªæ ·æœ¬/æ‰¹æ¬¡ï¼‰
            per_device_eval_batch_size=32,# â€‹æ¯ä¸ªGPU/CPUâ€‹â€‹ çš„éªŒè¯æ‰¹æ¬¡å¤§å°ï¼ˆéªŒè¯æ—¶å¯æ›´å¤§ä»¥åŠ é€Ÿï¼‰
            learning_rate=3e-5,# åˆå§‹å­¦ä¹ ç‡
            warmup_steps=100, # å‰100æ­¥é€æ­¥å¢åŠ å­¦ä¹ ç‡ï¼ˆé¿å…åˆå§‹è®­ç»ƒä¸ç¨³å®šï¼‰
            weight_decay=0.01,# L2æ­£åˆ™åŒ–ç³»æ•°ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
            logging_dir='./logs',# TensorBoardæ—¥å¿—ä¿å­˜è·¯å¾„
            logging_steps=50,# æ¯50ä¸ªè®­ç»ƒæ­¥è®°å½•ä¸€æ¬¡æ—¥å¿—ï¼ˆå¦‚lossã€å­¦ä¹ ç‡ï¼‰
            # evaluation_strategy="steps",# æŒ‰æ­¥æ•°ï¼ˆè€Œéè½®æ¬¡ï¼‰è¿›è¡ŒéªŒè¯
            # eval_steps=200,# æ¯200ä¸ªè®­ç»ƒæ­¥éªŒè¯ä¸€æ¬¡æ¨¡å‹æ€§èƒ½
            save_steps=500,# æ¯500æ­¥ä¿å­˜ä¸€æ¬¡æ¨¡å‹æ£€æŸ¥ç‚¹
            # load_best_model_at_end=True,# è®­ç»ƒç»“æŸæ—¶åŠ è½½éªŒè¯é›†ä¸Šæ€§èƒ½æœ€å¥½çš„æ¨¡å‹
            # metric_for_best_model="f1",# ç”¨F1åˆ†æ•°ï¼ˆè€Œéå‡†ç¡®ç‡ï¼‰ä½œä¸ºé€‰æ‹©æœ€ä½³æ¨¡å‹çš„æ ‡å‡†
        )

        # ç¬¬ä¸€é˜¶æ®µï¼šå†»ç»“BERTåº•å±‚å‚æ•°1~6
        logger.info("Stage 1: Generic sentiment fine-tuning (freezing lower layers)")
        for name, param in model.named_parameters():# éå†æ¨¡å‹ä¸­æ‰€æœ‰å¯è®­ç»ƒå‚æ•°ï¼ˆåŒ…æ‹¬æƒé‡å’Œåç½®ï¼‰ï¼Œè¿”å›å‚æ•°åå’Œå‚æ•°å¯¹è±¡
            if 'bert.encoder.layer' in name and int(name.split('.')[3]) < 6:  # å†»ç»“å‰6å±‚,â€‹â€‹'ç­›é€‰å±äºBERT encoderå±‚çš„å‚æ•°
                param.requires_grad = False # å…³é—­æ¢¯åº¦è®¡ç®—ï¼Œå†»ç»“è¯¥å‚æ•°ï¼ˆè®­ç»ƒæ—¶ä¸å†æ›´æ–°

        # åˆ›å»ºäº†ä¸€ä¸ª â€‹â€‹Hugging Face çš„ Trainerå®ä¾‹â€‹â€‹ï¼Œç”¨äºç®¡ç†å’Œæ‰§è¡ŒBERTæ¨¡å‹çš„ç¬¬ä¸€é˜¶æ®µè®­ç»ƒ
        trainer_stage1 = Trainer(
            model=model,# ä¼ å…¥è¦è®­ç»ƒçš„æ¨¡å‹å®ä¾‹ï¼ˆæ­¤å¤„æ˜¯å†»ç»“äº†å‰6å±‚çš„BERTåˆ†ç±»æ¨¡å‹ï¼‰
            args=training_args_stage1,# ç»‘å®šç¬¬ä¸€é˜¶æ®µçš„è®­ç»ƒå‚æ•°é…ç½®ï¼ˆå¦‚å­¦ä¹ ç‡ã€æ‰¹æ¬¡å¤§å°ç­‰ï¼‰
            train_dataset=tokenized_train,# ä¼ å…¥â€‹â€‹åˆ†è¯åçš„è®­ç»ƒé›†â€‹â€‹ï¼ˆé€šè¿‡tokenize_functioné¢„å¤„ç†ï¼‰
            eval_dataset=tokenized_eval,# ä¼ å…¥â€‹â€‹åˆ†è¯åçš„éªŒè¯é›†â€‹â€‹ï¼ˆé€šè¿‡tokenize_functioné¢„å¤„ç†ï¼‰
            tokenizer=self.tokenizer,# ç»‘å®šåˆ†è¯å™¨
            data_collator=data_collator,# æ€å°†æ‰¹æ¬¡å†…æ ·æœ¬å¡«å……åˆ°ç›¸åŒé•¿åº¦ï¼ˆä¼˜åŒ–æ˜¾å­˜ï¼‰
            compute_metrics=self.compute_metrics,# å®šä¹‰è¯„ä¼°æŒ‡æ ‡çš„è®¡ç®—æ–¹æ³•ï¼ˆå¦‚å‡†ç¡®ç‡ã€F1ï¼‰
        )

        # å¼€å§‹ç¬¬ä¸€é˜¶æ®µè®­ç»ƒ
        trainer_stage1.train()
        trainer_stage1.save_model()

        # ç¬¬äºŒé˜¶æ®µï¼šå…¨å‚æ•°å•†å“é¢†åŸŸå¾®è°ƒ
        logger.info("Stage 2: Domain-specific fine-tuning (unfreezing all layers)")
        
        # è§£å†»æ‰€æœ‰å‚æ•°
        for param in model.parameters():
            param.requires_grad = True

        # ç¬¬äºŒé˜¶æ®µè®­ç»ƒå‚æ•°ï¼ˆæ›´å°çš„å­¦ä¹ ç‡ï¼‰
        training_args_stage2 = TrainingArguments(
            output_dir=os.path.join(output_dir, "stage2"),
            # overwrite_output_dir=True,
            num_train_epochs=3,  # æ›´å¤šepochsä¸“æ³¨äºé¢†åŸŸé€‚åº”
            per_device_train_batch_size=16,
            per_device_eval_batch_size=32,
            learning_rate=5e-6,  # æ›´å°çš„å­¦ä¹ ç‡
            warmup_steps=50,
            weight_decay=0.01,
            logging_dir='./logs_stage2',
            logging_steps=50,
            # evaluation_strategy="steps",
            # eval_steps=200,
            save_steps=500,
            # load_best_model_at_end=True,
            # metric_for_best_model="f1",
        )

        trainer_stage2 = Trainer(
            model=model,
            args=training_args_stage2,
            train_dataset=tokenized_train,
            eval_dataset=tokenized_eval,
            tokenizer=self.tokenizer,
            data_collator=data_collator,
            compute_metrics=self.compute_metrics,
        )

        # å¼€å§‹ç¬¬äºŒé˜¶æ®µè®­ç»ƒ
        trainer_stage2.train()
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_model_path = os.path.join(output_dir, "final_model")
        trainer_stage2.save_model(final_model_path)
        self.tokenizer.save_pretrained(final_model_path)# å°†åˆ†è¯å™¨ä¿å­˜åˆ°ä¸æ¨¡å‹ç›¸åŒçš„ç›®å½•
        
        logger.info(f"Training completed! Model saved to: {final_model_path}")
        
        return trainer_stage2

    def predict(self, text, model_path=None):
        """ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹"""
        if model_path is None:
            model_path = "/home/zhp//bert/bert-sentiment-model/final_model"
            
        model = BertForSequenceClassification.from_pretrained(model_path,local_files_only=True)
        model.to(self.device)
        model.eval()
        
        inputs = self.tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
        inputs = {k: v.to(self.device) for k, v in inputs.items()}# å°†è¾“å…¥æ•°æ®ï¼ˆinput_ids, attention_maskï¼‰ç§»è‡³ä¸æ¨¡å‹ç›¸åŒçš„è®¾å¤‡ã€‚
        
        with torch.no_grad():
            outputs = model(**inputs)
            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
            
        label = torch.argmax(predictions, dim=1).item()
        confidence = predictions[0][label].item()# å°†è¾“å…¥æ•°æ®ï¼ˆinput_ids, attention_maskï¼‰ç§»è‡³ä¸æ¨¡å‹ç›¸åŒçš„è®¾å¤‡ã€‚
        
        sentiment = "æ­£é¢" if label == 1 else "è´Ÿé¢"
        
        return {
            "sentiment": sentiment,# # æƒ…æ„Ÿæ ‡ç­¾ï¼ˆä¸­æ–‡ï¼‰
            "label": label,# # æ•°å­—æ ‡ç­¾ï¼ˆ0/1ï¼‰
            "confidence": confidence,# # ç½®ä¿¡åº¦ï¼ˆ0~1ï¼‰
            "predictions": predictions.cpu().numpy()# åŸå§‹æ¦‚ç‡åˆ†å¸ƒ
        }

import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

def evaluate_model(test_dataset, analyzer, model_path="/home/zhp/bert/my-product-sentiment-model/final_model"):
    """
    ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œå…¨é¢æ€§èƒ½è¯„ä»·
    """
    logger.info("å¼€å§‹æ¨¡å‹æ€§èƒ½è¯„ä¼°...")
    
    # 1. å‡†å¤‡æµ‹è¯•æ•°æ®
    def prepare_test_data(example):
        """å‡†å¤‡æµ‹è¯•æ ·æœ¬"""
        return analyzer.tokenize_function(example)
    
    # å¯¹æµ‹è¯•é›†è¿›è¡Œtokenization
    tokenized_test = test_dataset.map(prepare_test_data, batched=True)
    
    # 2. åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
    logger.info("åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹...")
    model = BertForSequenceClassification.from_pretrained(
        model_path, 
        local_files_only=True
    )
    model.to(analyzer.device)
    model.eval()
    
    # 3. æ‰¹é‡é¢„æµ‹
    logger.info("åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œé¢„æµ‹...")
    all_predictions = []
    all_labels = []
    
    with torch.no_grad():
        for i in range(len(tokenized_test)):
            # è·å–å•ä¸ªæ ·æœ¬
            sample = tokenized_test[i]
            input_ids = torch.tensor(sample['input_ids']).unsqueeze(0).to(analyzer.device)
            attention_mask = torch.tensor(sample['attention_mask']).unsqueeze(0).to(analyzer.device)
            label = sample['label']
            
            # é¢„æµ‹
            outputs = model(input_ids, attention_mask=attention_mask)
            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
            predicted_label = torch.argmax(predictions, dim=1).item()
            
            all_predictions.append(predicted_label)
            all_labels.append(label)
            
            # è¿›åº¦æ˜¾ç¤º
            if (i + 1) % 100 == 0:
                logger.info(f"å·²å¤„ç† {i+1}/{len(tokenized_test)} ä¸ªæ ·æœ¬")
    
    # 4. è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    logger.info("è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
    y_true = np.array(all_labels)
    y_pred = np.array(all_predictions)
    
    # åŸºç¡€æŒ‡æ ‡
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, average='weighted')
    recall = recall_score(y_true, y_pred, average='weighted')
    f1 = f1_score(y_true, y_pred, average='weighted')
    
    # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
    class_report = classification_report(y_true, y_pred, target_names=['è´Ÿé¢', 'æ­£é¢'])
    
    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_true, y_pred)
    
    # 5. è¾“å‡ºç»“æœ
    logger.info("ğŸ¯ æ¨¡å‹æ€§èƒ½è¯„ä¼°ç»“æœ")
    logger.info("=" * 60)
    logger.info(f"ğŸ“Š æ•°æ®é›†ä¿¡æ¯:")
    logger.info(f"   æµ‹è¯•é›†æ ·æœ¬æ•°: {len(y_true)}")
    logger.info(f"   æ­£é¢æ ·æœ¬æ•°: {np.sum(y_true == 1)}")
    logger.info(f"   è´Ÿé¢æ ·æœ¬æ•°: {np.sum(y_true == 0)}")
    logger.info("")
    
    logger.info(f"ğŸ“ˆ æ ¸å¿ƒæŒ‡æ ‡:")
    logger.info(f"   å‡†ç¡®ç‡ (Accuracy): {accuracy:.4f}")
    logger.info(f"   ç²¾ç¡®ç‡ (Precision): {precision:.4f}")
    logger.info(f"   å¬å›ç‡ (Recall): {recall:.4f}")
    logger.info(f"   F1åˆ†æ•°: {f1:.4f}")
    logger.info("")
    
    logger.info(f"ğŸ“‹ è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
    logger.info(class_report)
    logger.info("")
    
    logger.info(f"ğŸ­ æ··æ·†çŸ©é˜µ:")
    logger.info(f"   TN: {cm[0,0]} | FP: {cm[0,1]}")
    logger.info(f"   FN: {cm[1,0]} | TP: {cm[1,1]}")
    logger.info("")
    
    # 6. ä¸šåŠ¡åœºæ™¯åˆ†æ
    logger.info(f"ğŸ’¼ ä¸šåŠ¡åœºæ™¯åˆ†æ:")
    negative_precision = cm[0,0] / (cm[0,0] + cm[1,0]) if (cm[0,0] + cm[1,0]) > 0 else 0
    positive_recall = cm[1,1] / (cm[1,0] + cm[1,1]) if (cm[1,0] + cm[1,1]) > 0 else 0
    
    logger.info(f"   è´Ÿé¢è¯„è®ºç²¾ç¡®ç‡: {negative_precision:.4f} (å‡å°‘è¯¯åˆ¤è´Ÿé¢ä¸ºæ­£é¢)")
    logger.info(f"   æ­£é¢è¯„è®ºå¬å›ç‡: {positive_recall:.4f} (ä¸é”™è¿‡ä»»ä½•æ­£é¢è¯„ä»·)")
    logger.info("=" * 60)
    
    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'y_true': y_true,
        'y_pred': y_pred,
        'confusion_matrix': cm
    }

def plot_confusion_matrix(cm, title='æ··æ·†çŸ©é˜µ'):
    """å¯è§†åŒ–æ··æ·†çŸ©é˜µ"""
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=['è´Ÿé¢', 'æ­£é¢'], 
                yticklabels=['è´Ÿé¢', 'æ­£é¢'])
    plt.title(title)
    plt.xlabel('é¢„æµ‹æ ‡ç­¾')
    plt.ylabel('çœŸå®æ ‡ç­¾')
    plt.show()

def benchmark_original_model(test_dataset):
    """
    æµ‹è¯•åˆå§‹ Bert-base-chinese æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½
    ä½œä¸ºä¼˜åŒ–å‰çš„åŸºå‡†å¯¹æ¯”
    """
    logger.info("ğŸ§ª å¼€å§‹åŸºå‡†æµ‹è¯•ï¼šåŸå§‹ Bert-base-chinese æ¨¡å‹")
    
    # 1. åŠ è½½åŸå§‹æœªå¾®è°ƒçš„æ¨¡å‹
    logger.info("åŠ è½½åŸå§‹ Bert-base-chinese æ¨¡å‹...")
    tokenizer = BertTokenizer.from_pretrained("bert-base-chinese")
    model = BertForSequenceClassification.from_pretrained(
        "bert-base-chinese", 
        num_labels=2
    )
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.eval()
    
    # 2. å‡†å¤‡æµ‹è¯•æ•°æ®
    def tokenize_function(examples):
        return tokenizer(examples["text"], truncation=True, padding=False, max_length=512)
    
    tokenized_test = test_dataset.map(tokenize_function, batched=True)
    
    # 3. é¢„æµ‹
    logger.info("åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œé¢„æµ‹...")
    all_predictions = []
    all_labels = []
    
    with torch.no_grad():
        for i in range(len(tokenized_test)):
            sample = tokenized_test[i]
            input_ids = torch.tensor(sample['input_ids']).unsqueeze(0).to(device)
            attention_mask = torch.tensor(sample['attention_mask']).unsqueeze(0).to(device)
            label = sample['label']
            
            outputs = model(input_ids, attention_mask=attention_mask)
            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
            predicted_label = torch.argmax(predictions, dim=1).item()
            
            all_predictions.append(predicted_label)
            all_labels.append(label)
            
            if (i + 1) % 100 == 0:
                logger.info(f"å·²å¤„ç† {i+1}/{len(tokenized_test)} ä¸ªæ ·æœ¬")
    
    # 4. è®¡ç®—æŒ‡æ ‡
    y_true = np.array(all_labels)
    y_pred = np.array(all_predictions)
    
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, average='weighted')
    recall = recall_score(y_true, y_pred, average='weighted')
    f1 = f1_score(y_true, y_pred, average='weighted')
    
    cm = confusion_matrix(y_true, y_pred)
    
    # 5. è¾“å‡ºåŸºå‡†ç»“æœ
    logger.info("ğŸ“Š åŸå§‹æ¨¡å‹åŸºå‡†æ€§èƒ½")
    logger.info("=" * 50)
    logger.info(f"å‡†ç¡®ç‡ (Accuracy): {accuracy:.4f}")
    logger.info(f"ç²¾ç¡®ç‡ (Precision): {precision:.4f}")
    logger.info(f"å¬å›ç‡ (Recall): {recall:.4f}")
    logger.info(f"F1åˆ†æ•°: {f1:.4f}")
    logger.info("")
    logger.info(f"æ··æ·†çŸ©é˜µ:")
    logger.info(f"TN: {cm[0,0]} | FP: {cm[0,1]}")
    logger.info(f"FN: {cm[1,0]} | TP: {cm[1,1]}")
    logger.info("=" * 50)
    
    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'confusion_matrix': cm,
        'y_true': y_true,
        'y_pred': y_pred
    }

def compare_models(original_results, optimized_results):
    """
    å¯¹æ¯”ä¼˜åŒ–å‰åçš„æ¨¡å‹æ€§èƒ½
    """
    logger.info("ğŸ”„ æ¨¡å‹ä¼˜åŒ–æ•ˆæœå¯¹æ¯”")
    logger.info("=" * 60)
    
    # æ€§èƒ½æå‡è®¡ç®—
    acc_improvement = optimized_results['accuracy'] - original_results['accuracy']
    f1_improvement = optimized_results['f1'] - original_results['f1']
    
    # æ€§èƒ½æå‡ç™¾åˆ†æ¯”
    acc_improvement_pct = (acc_improvement / original_results['accuracy']) * 100
    f1_improvement_pct = (f1_improvement / original_results['f1']) * 100
    
    logger.info("ğŸ“ˆ æ€§èƒ½å¯¹æ¯”è¡¨:")
    logger.info("æŒ‡æ ‡         | åŸå§‹æ¨¡å‹ | ä¼˜åŒ–æ¨¡å‹ | æå‡å€¼   | æå‡ç™¾åˆ†æ¯”")
    logger.info("-" * 60)
    logger.info(f"å‡†ç¡®ç‡      | {original_results['accuracy']:.4f} | {optimized_results['accuracy']:.4f} | +{acc_improvement:.4f} | +{acc_improvement_pct:.1f}%")
    logger.info(f"F1åˆ†æ•°      | {original_results['f1']:.4f} | {optimized_results['f1']:.4f} | +{f1_improvement:.4f} | +{f1_improvement_pct:.1f}%")
    logger.info(f"ç²¾ç¡®ç‡      | {original_results['precision']:.4f} | {optimized_results['precision']:.4f}")
    logger.info(f"å¬å›ç‡      | {original_results['recall']:.4f} | {optimized_results['recall']:.4f}")
    logger.info("")
    
    # ä¸šåŠ¡æŒ‡æ ‡å¯¹æ¯”
    orig_cm = original_results['confusion_matrix']
    opt_cm = optimized_results['confusion_matrix']
    
    orig_negative_precision = orig_cm[0,0] / (orig_cm[0,0] + orig_cm[1,0]) if (orig_cm[0,0] + orig_cm[1,0]) > 0 else 0
    opt_negative_precision = opt_cm[0,0] / (opt_cm[0,0] + opt_cm[1,0]) if (opt_cm[0,0] + opt_cm[1,0]) > 0 else 0
    
    logger.info("ğŸ’¼ ä¸šåŠ¡æŒ‡æ ‡å¯¹æ¯”:")
    logger.info(f"è´Ÿé¢è¯„è®ºç²¾ç¡®ç‡: {orig_negative_precision:.4f} â†’ {opt_negative_precision:.4f}")
    logger.info("=" * 60)
    
    return {
        'accuracy_improvement': acc_improvement,
        'f1_improvement': f1_improvement,
        'improvement_percentage': {
            'accuracy': acc_improvement_pct,
            'f1': f1_improvement_pct
        }
    }

def main():
    """ä¸»å‡½æ•°"""
    # 1. åŠ è½½æ¸…æ´—åçš„æ•°æ®
    # å‡è®¾ä½ çš„æ•°æ®å·²ç»ä¿å­˜ä¸ºHugging Faceæ•°æ®é›†æ ¼å¼
    try:
        # å¦‚æœä½ ç”¨ä¹‹å‰å»ºè®®çš„ save_to_disk ä¿å­˜äº†æ•°æ®
        dataset = load_from_disk('/home/zhp//bert/ChnSentiCorp_cleaned')
    except:
        # å¦‚æœæ•°æ®æ˜¯å…¶ä»–æ ¼å¼ï¼Œè¯·ä¿®æ”¹è¿™é‡Œçš„åŠ è½½æ–¹å¼
        raise ValueError("è¯·ç¡®ä¿æ•°æ®è·¯å¾„æ­£ç¡®ï¼Œä¸”æ•°æ®å·²æ¸…æ´—å¹¶å¯ç”¨")
    
    # æ£€æŸ¥æ•°æ®é›†ç»“æ„
    logger.info("Dataset structure:")
    for split in ['train', 'validation', 'test']:
        if split in dataset:
            logger.info(f"{split}: {len(dataset[split])} samples")
    
    # 2. åˆå§‹åŒ–åˆ†æå™¨
    analyzer = SentimentAnalyzer()
    
    # 3. å¼€å§‹è®­ç»ƒ
    trainer = analyzer.train(
        train_dataset=dataset['train'],
        eval_dataset=dataset['validation'],
        output_dir="/home/zhp//bert/my-product-sentiment-model"
    )
    
    # 4. åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æœ€ç»ˆæ¨¡å‹
    logger.info("Evaluating on test set...")
    test_results = trainer.evaluate(dataset['test'].map(analyzer.tokenize_function, batched=True))
    logger.info(f"Test results: {test_results}")
    
    logger.info("å¼€å§‹æ¨¡å‹æ€§èƒ½è¯„ä¼°...")
    evaluation_results = evaluate_model(
        test_dataset=dataset['test'], 
        analyzer=analyzer,
        model_path="/home/zhp/bert/my-product-sentiment-model/final_model"
    )
        
    # 4. å¯é€‰ï¼šå¯è§†åŒ–æ··æ·†çŸ©é˜µ
    plot_confusion_matrix(evaluation_results['confusion_matrix'])

    # 5. ç¤ºä¾‹é¢„æµ‹
    logger.info("Example predictions:")
    test_texts = [
        "ç‰©æµé€Ÿåº¦å¾ˆå¿«ï¼ŒåŒ…è£…å®Œå¥½ï¼Œå•†å“è´¨é‡ä¸é”™",
        "å®¢æœæ€åº¦å¾ˆå·®ï¼Œé€€è´§æµç¨‹å¤æ‚ï¼Œä¸æ¨èè´­ä¹°",
        "æ€§ä»·æ¯”ä¸€èˆ¬ï¼Œä¸æ¨èè´­ä¹°"
    ]
    
    for text in test_texts:
        result = analyzer.predict(text,model_path="/home/zhp//bert/my-product-sentiment-model/final_model")
        logger.info(f"æ–‡æœ¬: {text}")
        logger.info(f"æƒ…æ„Ÿ: {result['sentiment']} (ç½®ä¿¡åº¦: {result['confidence']:.4f})")
        logger.info("---")

    # 2. æµ‹è¯•åŸå§‹æ¨¡å‹æ€§èƒ½ï¼ˆåŸºå‡†ï¼‰
    logger.info("æ­¥éª¤1: æµ‹è¯•åŸå§‹ Bert-base-chinese æ¨¡å‹")
    original_results = benchmark_original_model(dataset['test'])
    
    # 3. æµ‹è¯•ä¼˜åŒ–åæ¨¡å‹æ€§èƒ½
    logger.info("æ­¥éª¤2: æµ‹è¯•ä¼˜åŒ–åçš„æ¨¡å‹")
    analyzer = SentimentAnalyzer()
    optimized_results = evaluate_model(
        test_dataset=dataset['test'], 
        analyzer=analyzer,
        model_path="/home/zhp/bert/my-product-sentiment-model/final_model"
    )
    
    # 4. æ€§èƒ½å¯¹æ¯”åˆ†æ
    logger.info("æ­¥éª¤3: æ€§èƒ½å¯¹æ¯”åˆ†æ")
    comparison = compare_models(original_results, optimized_results)
    
    # 5. ç»“è®º
    logger.info("ğŸ¯ ä¼˜åŒ–æ•ˆæœæ€»ç»“")
    if comparison['accuracy_improvement'] > 0.05:  # æå‡è¶…è¿‡5%
        logger.info("ğŸ† ä¼˜åŒ–æ•ˆæœï¼šéå¸¸æ˜¾è‘—ï¼")
    elif comparison['accuracy_improvement'] > 0.02:  # æå‡è¶…è¿‡2%
        logger.info("âœ… ä¼˜åŒ–æ•ˆæœï¼šæ˜æ˜¾æå‡ï¼")
    else:
        logger.info("âš ï¸ ä¼˜åŒ–æ•ˆæœï¼šæœ‰å¾…æ”¹è¿›")

    

if __name__ == "__main__":
    main()